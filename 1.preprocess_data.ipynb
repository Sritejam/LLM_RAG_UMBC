{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4b4e57c",
   "metadata": {},
   "source": [
    "1. ***Preprocessing:*** <br><tb>\n",
    "This notebook focuses on preparing raw UMBC ISSS data for the Retrieval-Augmented Generation (RAG) system.\n",
    "Key Steps:\n",
    "* Extract text from .txt and .docx files.\n",
    "* Clean the data by removing special characters and unwanted formatting.\n",
    "* Chunk text into ~60-word semantic segments, adding context-before and context-after metadata.\n",
    "* Save the processed chunks as JSON files for efficient retrieval.\n",
    "* Uploaded to Github: https://github.com/Aravind6908/datahub/tree/main/LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "602258bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dceda3a3-c731-4636-b895-4a1f46634694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\users\\aravi\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\aravi\\anaconda3\\lib\\site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\aravi\\anaconda3\\lib\\site-packages (from python-docx) (4.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e9a2baa-5e9f-4544-8a71-e81a5b8b93ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pywin32 in c:\\users\\aravi\\anaconda3\\lib\\site-packages (305.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pywin32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be04a069-2d9a-4efb-bd42-3c66639b3f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aravi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed About_OISS.docx: 3 chunks created.\n",
      "Processed Applying_to_UMBC.docx: 5 chunks created.\n",
      "Processed Career_Center.docx: 1 chunks created.\n",
      "Processed Change_of_Education_or_Degree_Level.docx: 8 chunks created.\n",
      "Processed Contact_Us.docx: 7 chunks created.\n",
      "Processed CPT.docx: 16 chunks created.\n",
      "Processed Economic_Hardship_Work_Authorization.docx: 16 chunks created.\n",
      "Processed Education_Abroad_for_International_Students.docx: 4 chunks created.\n",
      "Processed Entering_the_USA.docx: 20 chunks created.\n",
      "Processed F1_Enrollements_and_Requirements.docx: 35 chunks created.\n",
      "Processed F1_Visa_Process.docx: 35 chunks created.\n",
      "Processed Financing_Your_Studies.docx: 9 chunks created.\n",
      "Processed Graduate_Assistantships.docx: 13 chunks created.\n",
      "Processed H1-B_Overview.docx: 28 chunks created.\n",
      "Processed Health_Insurance_and_Immunization_Requirements.docx: 6 chunks created.\n",
      "Processed I-20_Program_Extension.docx: 6 chunks created.\n",
      "Processed i20.docx: 15 chunks created.\n",
      "Processed i20_Access.docx: 6 chunks created.\n",
      "Processed Instructor_Schedules.docx: 2 chunks created.\n",
      "Processed International_Student_Orientation.docx: 3 chunks created.\n",
      "Processed In_State_Tuition_Fees.docx: 1 chunks created.\n",
      "Processed Loss_and_Reinstatement_of_F1.docx: 10 chunks created.\n",
      "Processed Meet_the_Staff.docx: 5 chunks created.\n",
      "Processed OISS_Staff.docx: 1 chunks created.\n",
      "Processed On_Campus_work.docx: 5 chunks created.\n",
      "Processed OPT.docx: 22 chunks created.\n",
      "Processed OPT_and_STEM_OPT_Best_Practices _for_H1B.docx: 17 chunks created.\n",
      "Processed Packing_Tips.docx: 11 chunks created.\n",
      "Processed Passport.docx: 3 chunks created.\n",
      "Processed PhD_Student_OPT_Planning.docx: 8 chunks created.\n",
      "Processed SEVP_Portal_Help.docx: 28 chunks created.\n",
      "Processed Start_Planning_to_Arrive_at_UMBC.docx: 5 chunks created.\n",
      "Processed STEM_OPT_Extension.docx: 29 chunks created.\n",
      "Processed Studying_in_the_USA.docx: 3 chunks created.\n",
      "Processed Taxes_for_students_on_OPT.docx: 7 chunks created.\n",
      "Processed Transfer_i20.docx: 12 chunks created.\n",
      "Processed Transfer_out_during_OPT.docx: 3 chunks created.\n",
      "Processed Travel_Abroad_and_Visa_Renewal.docx: 25 chunks created.\n",
      "Processed Travel_Abroad_and_Visa_Renewal_During_OPT.docx: 10 chunks created.\n",
      "Processed Tuition_fees.docx: 11 chunks created.\n",
      "Processed Visiting_the_USA_on_B-2_Visitor_Visa.docx: 3 chunks created.\n",
      "Processed Welcome_to_CGE.docx: 10 chunks created.\n",
      "Processed Welcome_to_ISSS.docx: 5 chunks created.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from docx import Document  \n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Directories\n",
    "input_directory = \"./documents\"  # Input folder\n",
    "output_directory = \"./pre_processed_documents\"  # Output folder\n",
    "\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "def clean_text(text, preserve_words=None):\n",
    "    \n",
    "    if preserve_words is None:\n",
    "        preserve_words = {\"US\", \"OPT\", \"I-20\", \"I-94\"}  \n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text)  \n",
    "    text = re.sub(r'<.*?>', '', text)  \n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text) \n",
    "    text = re.sub(r'[^A-Za-z0-9@.,!?\\'\\\";:\\s\\-$]', '', text)\n",
    "    words = text.split()\n",
    "    processed_words = [\n",
    "        word if word.upper() in preserve_words else word.lower()\n",
    "        for word in words\n",
    "    ]\n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "\n",
    "\n",
    "# Function to chunk text semantically based on sentences and token limits\n",
    "def semantic_chunking(text, max_token_size=512):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = len(sentence.split())\n",
    "        if current_tokens + sentence_tokens <= max_token_size:\n",
    "            current_chunk.append(sentence)\n",
    "            current_tokens += sentence_tokens\n",
    "        else:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_tokens = sentence_tokens\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "# Function to add context to chunks\n",
    "def add_context(chunks, context_window=1):\n",
    "    contextual_chunks = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        context_before = ' '.join(chunks[max(i - context_window, 0):i])\n",
    "        context_after = ' '.join(chunks[i + 1:i + 1 + context_window])\n",
    "        contextual_chunks.append({\n",
    "            \"chunk\": chunk,\n",
    "            \"context_before\": context_before,\n",
    "            \"context_after\": context_after\n",
    "        })\n",
    "    return contextual_chunks\n",
    "\n",
    "# Function to extract text from .docx files\n",
    "def extract_text_from_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    return \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
    "\n",
    "# Processing each file\n",
    "for filename in os.listdir(input_directory):\n",
    "    file_path = os.path.join(input_directory, filename)\n",
    "\n",
    "    try:\n",
    "        # Handling different file types\n",
    "        if filename.lower().endswith('.txt'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                raw_text = file.read()\n",
    "        elif filename.lower().endswith('.docx'):\n",
    "            raw_text = extract_text_from_docx(file_path)\n",
    "        else:\n",
    "            print(f\"Skipping unsupported file type: {filename}\")\n",
    "            continue\n",
    "\n",
    "        # Cleaning and preprocessing the text\n",
    "        cleaned_text = clean_text(raw_text)\n",
    "        chunks = semantic_chunking(cleaned_text, max_token_size=60)\n",
    "        contextual_chunks = add_context(chunks, context_window=1)\n",
    "        for idx, chunk_data in enumerate(contextual_chunks):\n",
    "            output_path = os.path.join(output_directory, f\"{Path(filename).stem}_chunk{idx}.json\")\n",
    "            chunk_data_with_metadata = {\n",
    "                \"DocumentID\": filename,\n",
    "                \"ChunkID\": idx,\n",
    "                \"ChunkText\": chunk_data[\"chunk\"],\n",
    "                \"ContextBefore\": chunk_data[\"context_before\"],\n",
    "                \"ContextAfter\": chunk_data[\"context_after\"]\n",
    "            }\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "                json.dump(chunk_data_with_metadata, output_file, indent=4)\n",
    "\n",
    "        print(f\"Processed {filename}: {len(contextual_chunks)} chunks created.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfdda82-7052-43be-bae2-77f6856a8a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
